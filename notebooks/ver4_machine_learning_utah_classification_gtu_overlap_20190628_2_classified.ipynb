{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "import numpy as np\n",
    "import psycopg2 as pg\n",
    "import pandas as pd\n",
    "import pandas.io.sql as psql\n",
    "import getpass\n",
    "import matplotlib as mpl\n",
    "import argparse\n",
    "import glob\n",
    "import traceback\n",
    "import hashlib\n",
    "import math\n",
    "import collections\n",
    "import functools\n",
    "\n",
    "from statsmodels.stats.proportion import proportion_confint as statmodels_proportion_confint\n",
    "\n",
    "# import sklearn.preprocessing\n",
    "# import sklearn.feature_selection\n",
    "# import sklearn.ensemble \n",
    "# import sklearn.neural_network\n",
    "# import sklearn.model_selection\n",
    "# import sklearn.metrics\n",
    "# import sklearn.pipeline\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 80\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import IPython.display\n",
    "from slugify import slugify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_base_dir = '/home/spbproc/euso-spb-patt-reco-v1'\n",
    "if app_base_dir not in sys.path:\n",
    "    sys.path.append(app_base_dir)\n",
    "\n",
    "import event_processing_v3\n",
    "import event_processing_v4\n",
    "import postgresql_v3_event_storage\n",
    "import dataset_query_functions_v3\n",
    "\n",
    "import tool.acqconv\n",
    "from data_analysis_utils import *\n",
    "from data_analysis_utils_dataframes import *\n",
    "from data_analysis_utils_performance import *\n",
    "# import supervised_classification as supc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse_means_map = np.load('/home/spbproc/euso-spb-patt-reco-v1/resources/inverse_flat_average_directions_4m_flipud.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_id = '20190628_2'\n",
    "model_data_snippets_dir = 'ver4_machine_learning_w_labeled_flight_' + classification_id\n",
    "utah_file_analysis_snippets_dir = 'utah_events_directory_analysis'\n",
    "data_snippets_dir = 'ver4_machine_learning_utah_classification_gtu_overlap_' + classification_id\n",
    "\n",
    "subset_classification_slug = '_v4_ml_go_' + classification_id\n",
    "# event_classification_v4_ml_go_20190628_2\n",
    "\n",
    "os.makedirs(data_snippets_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_processing_cls = event_processing_v4.EventProcessingV4\n",
    "event_v3_storage_provider_utah = dataset_query_functions_v3.build_event_v3_storage_provider(\n",
    "    event_storage_provider_config_file=os.path.join(app_base_dir,'config.ini'), \n",
    "    table_names_version='ver4',\n",
    "    event_storage_class=postgresql_v3_event_storage.PostgreSqlEventV3StorageProvider,\n",
    "    event_processing_class=event_processing_cls,\n",
    "    readonly=False,\n",
    "    schema_name_overwrite = 'miso2_2_spb_processing_v4'\n",
    ")\n",
    "\n",
    "query_functions_utah = dataset_query_functions_v3.Ver3DatasetQueryFunctions(event_v3_storage_provider_utah)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv_selector_on_extra_trees__column_names = []\n",
    "\n",
    "columns_list_file_pathname = os.path.join(model_data_snippets_dir, 'rfecv_selector_on_extra_trees__column_names.txt')\n",
    "print(columns_list_file_pathname)\n",
    "with open(columns_list_file_pathname, 'r') as columns_list_file:\n",
    "    rfecv_selector_on_extra_trees__column_names = columns_list_file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv_selector_on_extra_trees__column_names__special = []\n",
    "\n",
    "special_columns_list_file_pathname = os.path.join(model_data_snippets_dir, 'rfecv_selector_on_extra_trees__column_names__special.txt')\n",
    "print(special_columns_list_file_pathname)\n",
    "with open(special_columns_list_file_pathname, 'r') as special_columns_list_file:\n",
    "    rfecv_selector_on_extra_trees__column_names__special = special_columns_list_file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be empty for now\n",
    "rfecv_selector_on_extra_trees__column_names__special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utah_columns_for_analysis_dict = query_functions_utah.get_columns_for_classification_dict__by_excluding(\n",
    "    excluded_columns_re_list=('^.+$',),\n",
    "    default_excluded_columns_re_list=[],\n",
    "    included_columns_re_list=[('^$','source_file_(acquisition|trigger)(_full)?|global_gtu|packet_id|gtu_in_packet|event_id|num_gtu'),] + rfecv_selector_on_extra_trees__column_names\n",
    ")\n",
    "\n",
    "classification_utah_columns_for_analysis_dict = query_functions_utah.get_columns_for_classification_dict__by_excluding(\n",
    "    excluded_columns_re_list=('^.+$',),\n",
    "    default_excluded_columns_re_list=[],\n",
    "    included_columns_re_list=rfecv_selector_on_extra_trees__column_names\n",
    ")\n",
    "\n",
    "print_columns_dict(utah_columns_for_analysis_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: not selecting NULL trg lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_columns_for_analysis_dict = utah_columns_for_analysis_dict\n",
    "\n",
    "utah_select_clause_str, utah_tables_list = \\\n",
    "    query_functions_utah.get_query_clauses__select({\n",
    "    **current_columns_for_analysis_dict,\n",
    "})\n",
    "\n",
    "utah_clauses_str = \\\n",
    "    query_functions_utah.get_query_clauses__join(utah_tables_list)\n",
    "\n",
    "utah_source_data_type_num = 2\n",
    "\n",
    "utah_where_clauses_str = ''\n",
    "# ''' \n",
    "#     AND abs(gtu_in_packet-42) < 20\n",
    "#     AND {database_schema_name}.event_orig_x_y.count_nonzero > 256*6\n",
    "# '''\n",
    "\n",
    "for table, cols_list in classification_utah_columns_for_analysis_dict.items():\n",
    "    for col in cols_list:\n",
    "        utah_where_clauses_str += ' AND {}.{} IS NOT NULL\\n'.format(table, col)\n",
    "\n",
    "utah_events_selection_query = query_functions_utah.get_events_selection_query_plain(\n",
    "    source_data_type_num=utah_source_data_type_num,\n",
    "    select_additional=utah_select_clause_str, \n",
    "    join_additional=utah_clauses_str,\n",
    "    where_additional=utah_where_clauses_str,\n",
    "    order_by='{data_table_name}.event_id', \n",
    "    offset=0, limit=1000000,\n",
    "    base_select='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(utah_events_selection_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "utah_df = psql.read_sql(utah_events_selection_query, event_v3_storage_provider_utah.connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(utah_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utah_df['dist_gtu_40'] = np.abs(utah_df['gtu_in_packet'] - 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "utah_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utah_df[rfecv_selector_on_extra_trees__column_names].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(utah_df['event_id'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utah_df['had_nan_fields'] = utah_df[rfecv_selector_on_extra_trees__column_names].isnull().any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(utah_df['had_nan_fields'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utah_df_nonan = utah_df[~utah_df['had_nan_fields']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(utah_df_nonan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# SELECT COUNT(*) FROM spb_processing_v4_flatmap.event JOIN spb_processing_v4_flatmap.event_orig_x_y USING(event_id) WHERE source_data_type_num = 1 AND abs(gtu_in_packet-42) < 20 AND spb_processing_v4_flatmap.event_orig_x_y.count_nonzero > 256*6 LIMIT 5;\n",
    "# SELECT COUNT( DISTINCT (source_file_acquisition, packet_id)) FROM spb_processing_v4_flatmap.event JOIN spb_processing_v4_flatmap.event_orig_x_y USING(event_id) WHERE source_data_type_num = 1 AND abs(gtu_in_packet-42) < 20 AND spb_processing_v4_flatmap.event_orig_x_y.count_nonzero > 256*6 LIMIT 5;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utah_df_nonan.gtu_in_packet.hist(bins=128+1, figsize=(10,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model does not use scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard_scaler_on_train_rfecv_columns_pathname = \\\n",
    "#      os.path.join(model_data_snippets_dir, 'standard_scaler_on_train_rfecv_columns.pkl')\n",
    "# standard_scaler_on_train_rfecv_columns = joblib.load(standard_scaler_on_train_rfecv_columns_pathname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flight__rfecv_columns_scaled_X = \\\n",
    "#     standard_scaler_on_train_rfecv_columns.transform(\n",
    "#         flight_df[rfecv_selector_on_extra_trees__column_names].values)\n",
    "# if np.count_nonzero(flight_df['had_nan_fields']) > 0:\n",
    "#     flight_nonan__rfecv_columns_scaled_X = \\\n",
    "#         standard_scaler_on_train_rfecv_columns.transform(\n",
    "#             flight_df_nonan[rfecv_selector_on_extra_trees__column_names].values)\n",
    "# else:\n",
    "#     flight_nonan__rfecv_columns_scaled_X = flight__rfecv_columns_scaled_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra trees classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utah_rfecv_columns__X  = utah_df_nonan[rfecv_selector_on_extra_trees__column_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_trees_cls_on_train_rfecv__model_plk_pathname = \\\n",
    "    os.path.join(model_data_snippets_dir, 'extra_trees_cls_on_train_rfecv.pkl')\n",
    "extra_trees_cls_on_train_rfecv_est = joblib.load(extra_trees_cls_on_train_rfecv__model_plk_pathname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO RENAME\n",
    "#       - extc_trn_rfecv_dn_est\n",
    "#       - extc_trn_rfecv_dn_proba\n",
    "\n",
    "cls_column_base = 'extra_trees_cls_on_train_rfecv_est'\n",
    "cls_column = cls_column_base + '_dropna'\n",
    "cls_proba_column = 'extra_trees_cls_on_train_rfecv_est_dropna_proba'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this might not be correct (but for this particular selection it should be fine becaus utah_df_nonan == utah_df )\n",
    "utah_df[cls_column_base] = \\\n",
    "    extra_trees_cls_on_train_rfecv_est.predict(utah_rfecv_columns__X)\n",
    "\n",
    "utah_df[cls_column] = \\\n",
    "    ((utah_df[cls_column_base]==1) & ~utah_df['had_nan_fields']).astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utah_df[cls_proba_column] = np.nan\n",
    "utah_df.loc[utah_df['event_id'].isin(utah_df_nonan['event_id']), cls_proba_column] = \\\n",
    "    extra_trees_cls_on_train_rfecv_est.predict_proba(utah_rfecv_columns__X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,4))\n",
    "# , bins=50+1, range=(20, 70)\n",
    "h = ax.hist(utah_df_nonan.gtu_in_packet, bins=128+1, range=(0,128), label='All data')\n",
    "ax.hist(utah_df_nonan[utah_df[cls_column] == 1].gtu_in_packet, bins=h[1], label='Classified \"air shower\"')\n",
    "ax.legend()\n",
    "fig.savefig(os.path.join(data_snippets_dir,'gtu_in_packet_hist,svg'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating `flight_df_nonan` with classification predictions\n",
    "(not in the report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utah_df_nonan = utah_df[~utah_df['had_nan_fields']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics of selected events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.count_nonzero(utah_df[cls_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(utah_df[cls_column])/len(utah_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utah_df[[cls_column, cls_proba_column]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for proba in np.arange(0.5, 1.0, 0.1):\n",
    "    print('p > {:.2f}: {}'.format(proba, np.count_nonzero(utah_df[cls_proba_column] > proba)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utah_df_nonan[rfecv_selector_on_extra_trees__column_names].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving utah data into tsv\n",
    "(not in the report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_utah_data_dump_file = True\n",
    "overwrite_utah_data_dump_file = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if save_utah_data_dump_file:\n",
    "#     utah_data_tsv_pathname = os.path.join(data_snippets_dir, 'utah_data.tsv.gz')\n",
    "\n",
    "#     if overwrite_utah_data_dump_file or not os.path.exists(utah_data_tsv_pathname):\n",
    "#         print('Saving', utah_data_tsv_pathname)\n",
    "#         utah_df.to_csv(utah_data_tsv_pathname, sep='\\t', compression='gzip')\n",
    "#     else:\n",
    "#         print('Already exists', utah_data_tsv_pathname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shower_pred_by_proba_desc = utah_df_nonan[utah_df_nonan[cls_column] == 1].sort_values(cls_proba_column, ascending=False)\n",
    "shower_pred_by_proba_asc = utah_df_nonan[utah_df_nonan[cls_column] == 1].sort_values(cls_proba_column, ascending=True)\n",
    "noise_pred_by_proba_desc = utah_df_nonan[utah_df_nonan[cls_column] == 0].sort_values(cls_proba_column, ascending=False)\n",
    "noise_pred_by_proba_asc = utah_df_nonan[utah_df_nonan[cls_column] == 0].sort_values(cls_proba_column, ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of tracks - air shower prediction - sorted by probability descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_events_df(\n",
    "    shower_pred_by_proba_desc, \n",
    "    events_per_figure=10, max_figures=1, vis_gtux=True, vis_gtuy=True, \n",
    "    close_after_vis=False, show=True, \n",
    "    additional_printed_columns=[cls_proba_column, 'source_file_acquisition'],\n",
    "    by_one=True,\n",
    "    extension_func=None,\n",
    "    single_proj_width=4, single_proj_height=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_events_df(\n",
    "    shower_pred_by_proba_desc[1000:], \n",
    "    events_per_figure=10, max_figures=1, vis_gtux=True, vis_gtuy=True, \n",
    "    close_after_vis=False, show=True, \n",
    "    additional_printed_columns=[cls_proba_column, 'source_file_acquisition'],\n",
    "    by_one=True,\n",
    "    extension_func=None,\n",
    "    single_proj_width=4, single_proj_height=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_events_df(\n",
    "    shower_pred_by_proba_desc[5000:], \n",
    "    events_per_figure=10, max_figures=1, vis_gtux=True, vis_gtuy=True, \n",
    "    close_after_vis=False, show=True, \n",
    "    additional_printed_columns=[cls_proba_column, 'source_file_acquisition'],\n",
    "    by_one=True,\n",
    "    extension_func=None,\n",
    "    single_proj_width=4, single_proj_height=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_events_df(\n",
    "    shower_pred_by_proba_desc[10000:], \n",
    "    events_per_figure=10, max_figures=1, vis_gtux=True, vis_gtuy=True, \n",
    "    close_after_vis=False, show=True, \n",
    "    additional_printed_columns=[cls_proba_column, 'source_file_acquisition'],\n",
    "    by_one=True,\n",
    "    extension_func=None,\n",
    "    single_proj_width=4, single_proj_height=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_events_df(\n",
    "    shower_pred_by_proba_desc[20000:], \n",
    "    events_per_figure=10, max_figures=1, vis_gtux=True, vis_gtuy=True, \n",
    "    close_after_vis=False, show=True, \n",
    "    additional_printed_columns=[cls_proba_column, 'source_file_acquisition'],\n",
    "    by_one=True,\n",
    "    extension_func=None,\n",
    "    single_proj_width=4, single_proj_height=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_events_df(\n",
    "    shower_pred_by_proba_desc[30000:], \n",
    "    events_per_figure=10, max_figures=1, vis_gtux=True, vis_gtuy=True, \n",
    "    close_after_vis=False, show=True, \n",
    "    additional_printed_columns=[cls_proba_column, 'source_file_acquisition'],\n",
    "    by_one=True,\n",
    "    extension_func=None,\n",
    "    single_proj_width=4, single_proj_height=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_events_df(\n",
    "    shower_pred_by_proba_desc[40000:], \n",
    "    events_per_figure=10, max_figures=1, vis_gtux=True, vis_gtuy=True, \n",
    "    close_after_vis=False, show=True, \n",
    "    additional_printed_columns=[cls_proba_column],\n",
    "    by_one=True,\n",
    "    extension_func=None,\n",
    "    single_proj_width=4, single_proj_height=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of tracks - air shower prediction - sorted by probability ascending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shower_pred_by_proba_asc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_events_df(\n",
    "    shower_pred_by_proba_asc[:20], \n",
    "    events_per_figure=10, max_figures=1, vis_gtux=True, vis_gtuy=True, \n",
    "    close_after_vis=False, show=True, \n",
    "    additional_printed_columns=[cls_proba_column, 'source_file_acquisition'],\n",
    "    by_one=True,\n",
    "    extension_func=None,\n",
    "    single_proj_width=4, single_proj_height=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_events_df(\n",
    "    shower_pred_by_proba_asc[4000:], \n",
    "    events_per_figure=10, max_figures=1, vis_gtux=True, vis_gtuy=True, \n",
    "    close_after_vis=False, show=True, \n",
    "    additional_printed_columns=[cls_proba_column, 'source_file_acquisition'],\n",
    "    by_one=True,\n",
    "    extension_func=None,\n",
    "    single_proj_width=4, single_proj_height=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_events_df(\n",
    "    shower_pred_by_proba_asc[8000:], \n",
    "    events_per_figure=10, max_figures=1, vis_gtux=True, vis_gtuy=True, \n",
    "    close_after_vis=False, show=True, \n",
    "    additional_printed_columns=[cls_proba_column, 'source_file_acquisition'],\n",
    "    by_one=True,\n",
    "    extension_func=None,\n",
    "    single_proj_width=4, single_proj_height=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_events_df(\n",
    "    shower_pred_by_proba_asc[10000:], \n",
    "    events_per_figure=10, max_figures=1, vis_gtux=True, vis_gtuy=True, \n",
    "    close_after_vis=False, show=True, \n",
    "    additional_printed_columns=[cls_proba_column, 'source_file_acquisition'],\n",
    "    by_one=True,\n",
    "    extension_func=None,\n",
    "    single_proj_width=4, single_proj_height=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_events_df(\n",
    "    shower_pred_by_proba_asc[10000:].query('dist_gtu_40 < 5'), \n",
    "    events_per_figure=10, max_figures=1, vis_gtux=True, vis_gtuy=True, \n",
    "    close_after_vis=False, show=True, \n",
    "    additional_printed_columns=[cls_proba_column, 'source_file_acquisition'],\n",
    "    by_one=True,\n",
    "    extension_func=None,\n",
    "    single_proj_width=4, single_proj_height=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of tracks - noise prediction - sorted by probability descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(noise_pred_by_proba_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_events_df(\n",
    "    noise_pred_by_proba_desc, \n",
    "    events_per_figure=10, max_figures=1, vis_gtux=True, vis_gtuy=True, \n",
    "    close_after_vis=False, show=True, \n",
    "    additional_printed_columns=[cls_proba_column, 'source_file_acquisition'],\n",
    "    by_one=True,\n",
    "    extension_func=None,\n",
    "    single_proj_width=4, single_proj_height=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shower events (20190628)**:\n",
    " - 676813 strong track  - should not been rejected\n",
    "\n",
    "Shower events (20190409):\n",
    " - 1929 (short track, looks as single gtu track)\n",
    " \n",
    " 661289"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_events_df(\n",
    "    noise_pred_by_proba_desc[100:], \n",
    "    events_per_figure=10, max_figures=1, vis_gtux=True, vis_gtuy=True, \n",
    "    close_after_vis=False, show=True, \n",
    "    additional_printed_columns=[cls_proba_column, 'source_file_acquisition_full'],\n",
    "    by_one=True,\n",
    "    extension_func=None,\n",
    "    single_proj_width=4, single_proj_height=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shower events (20190409):\n",
    " - 686894 (obvious air shower track)\n",
    " - 316757 (short obvious air shower track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_events_df(\n",
    "    noise_pred_by_proba_desc[200000:], \n",
    "    events_per_figure=10, max_figures=1, vis_gtux=True, vis_gtuy=True, \n",
    "    close_after_vis=False, show=True, \n",
    "    additional_printed_columns=[cls_proba_column, 'source_file_acquisition_full'],\n",
    "    by_one=True,\n",
    "    extension_func=None,\n",
    "    single_proj_width=4, single_proj_height=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_events_df(\n",
    "    noise_pred_by_proba_desc[100000:], \n",
    "    events_per_figure=10, max_figures=1, vis_gtux=True, vis_gtuy=True, \n",
    "    close_after_vis=False, show=True, \n",
    "    additional_printed_columns=[cls_proba_column, 'source_file_acquisition_full'],\n",
    "    by_one=True,\n",
    "    extension_func=None,\n",
    "    single_proj_width=4, single_proj_height=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numbers of selected packets from eusospb analisi table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eusospb_analisi_with_pathnames_left_pathname = os.path.join(utah_file_analysis_snippets_dir, 'eusospb_analisi_with_pathnames_left.tsv')\n",
    "eusospb_analisi_with_pathnames_left_df = pd.read_csv(eusospb_analisi_with_pathnames_left_pathname, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eusospb_analisi_with_pathnames_left_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = eusospb_analisi_with_pathnames_left_df\n",
    "pack_max_100 = np.min([df['pack'], np.ones(len(df))*100], axis=0)\n",
    "\n",
    "for cfg in ['ta_euso', 'ta_euso_10', 'euso_bal', 'euso_bal_10', 'euso_bal_20']:\n",
    "    df['max_' + cfg] = np.max([df[cfg + '_ec2'], df[cfg + '_ec5'], df[cfg + '_ec8']], axis=0)\n",
    "    df['eff_' + cfg] = df['max_' + cfg] / pack_max_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eusospb_analisi_with_pathnames_left_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counts of recognized showers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# investigation of track gtu range - reuslt: 30 < gtu_in_packet < 68\n",
    "#3793 5 32\n",
    "# vis_events_df(\n",
    "# #     utah_df_nonan.query('gtu_in_packet == 68').sort_values(cls_proba_column, ascending=False), \n",
    "#     utah_df_nonan.query('event_id == 364599'),\n",
    "#     events_per_figure=10, max_figures=1, vis_gtux=True, vis_gtuy=True, \n",
    "#     close_after_vis=False, show=True, \n",
    "#     additional_printed_columns=[cls_proba_column, 'source_file_acquisition_full'],\n",
    "#     by_one=True,\n",
    "#     extension_func=None,\n",
    "#     single_proj_width=4, single_proj_height=3\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_events_df(\n",
    "    utah_df.query('event_id == 735810'), \n",
    "    events_per_figure=10, max_figures=1, vis_gtux=True, vis_gtuy=True, \n",
    "    close_after_vis=False, show=True, \n",
    "    additional_printed_columns=[cls_proba_column, 'source_file_acquisition_full'],\n",
    "    by_one=True,\n",
    "    extension_func=None,\n",
    "    single_proj_width=4, single_proj_height=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gtu_40_pack_range = (34, 50) # 30, 68\n",
    "\n",
    "\n",
    "df = eusospb_analisi_with_pathnames_left_df\n",
    "\n",
    "for class_num, class_label in enumerate(['noise', 'shower']):\n",
    "    df['num_' + class_label + '_events'] = -1\n",
    "    df['num_' + class_label + '_events_gtu_40'] = -1\n",
    "    df['num_' + class_label + '_packets'] = -1\n",
    "    df['num_' + class_label + '_packets_gtu_40'] = -1\n",
    "        \n",
    "for k, file_pathname in df['file_pathname'].items():\n",
    "    print(file_pathname)\n",
    "    for class_num, class_label in enumerate(['noise', 'shower']):\n",
    "\n",
    "        \n",
    "#         df['num_' + class_label + '_multiple_event_packets'] = -1\n",
    "#         df['num_' + class_label + '_multiple_packet_events'] = -1\n",
    "#         df['num_' + class_label + '_multiple_packet_events_gtu_40'] = -1\n",
    "\n",
    "        cls_events = utah_df_nonan[(utah_df_nonan['source_file_acquisition'] == file_pathname) & (utah_df_nonan[cls_column] == class_num)]\n",
    "        \n",
    "        cls_packets_num_events = cls_events.groupby('packet_id')['event_id'].count()\n",
    "        num_packets = len(cls_packets_num_events)\n",
    "        \n",
    "        cls_events_gtu_40 = cls_events[\n",
    "            ((cls_events.gtu_in_packet -4 + cls_events.num_gtu) > 40)  & \n",
    "            (cls_events.gtu_in_packet <= 45)\n",
    "         ]\n",
    "        \n",
    "#         cls_events_gtu_40_pack = cls_events_gtu_40 \\\n",
    "#             .reset_index() \\\n",
    "#             .sort_values(['packet_id', 'dist_gtu_40'], ascending=True) \\\n",
    "#             .groupby('packet_id', as_index=False).first() \\\n",
    "#             .set_index('index')\n",
    "        \n",
    "        # number of events in gtu 40 packet\n",
    "        cls_packets_num_events_gtu_40 = cls_events_gtu_40.groupby('packet_id')['event_id'].count()\n",
    "        # number of packets\n",
    "        num_packets_gtu_40 = len(cls_packets_num_events_gtu_40)\n",
    "        # packets ids where thre are multiple suitable events around gtu 40\n",
    "        multple_event_packets_gtu_40_indexes = cls_packets_num_events_gtu_40[cls_packets_num_events_gtu_40 > 1].index\n",
    "        \n",
    "        if(len(multple_event_packets_gtu_40_indexes) > 1):\n",
    "            print('MULTIPLE EVENTS IN A SINGLE PACKET\\n\\t{}({}) / {}({}): {}'.format(\n",
    "                class_label, class_num, file_pathname, k, len(multple_event_packets_gtu_40_indexes),\n",
    "                # 'cls_packets_num_events_gtu_40[cls_packets_num_events_gtu_40 > 1]'\n",
    "            ))\n",
    "        \n",
    "        print('\\t{:<6} tot_pack={:<3d} gtu_40_pack={:<3d} pack={:<3d} max_euso_bal={:<3d} cls_events={:<4d} cls_events_gtu_40={:<4d}'.format(\n",
    "            class_label,  num_packets, num_packets_gtu_40, df.loc[k].pack, df.loc[k].max_euso_bal, len(cls_events), len(cls_events_gtu_40)))\n",
    "        \n",
    "        \n",
    "        if class_label == 'shower' and len(cls_packets_num_events) > len(cls_packets_num_events_gtu_40):\n",
    "#             print(cls_packets_num_events.index)\n",
    "#             print(cls_packets_num_events_gtu_40.index)\n",
    "            gtu_40_missing_packets = cls_packets_num_events[~cls_packets_num_events.index.isin(cls_packets_num_events_gtu_40.index)]\n",
    "            \n",
    "            cls_events_outside_gtu_40_df = cls_events[cls_events.packet_id.isin(gtu_40_missing_packets.index)]\n",
    "            \n",
    "            print('\\tOUTSIDE GTU=40\\t{}({}): num_out={}'.format(\n",
    "                class_label, class_num, len(cls_events_outside_gtu_40_df)\n",
    "            ))\n",
    "            print('\\t\\t{:<8} {:<3} {:<5} {:<3} {}'.format('event_id', 'pck', 'gtu.p', 'num', 'proba'))\n",
    "            for ti, (tk, tr) in enumerate(cls_events_outside_gtu_40_df.sort_values('dist_gtu_40').iterrows()):\n",
    "                print('\\t\\t{:<8} {:<3} {:<5} {:<3} {:.2f}'.format(tr['event_id'], tr['packet_id'], tr['gtu_in_packet'], tr['num_gtu'], tr[cls_proba_column]))\n",
    "                if ti > 10:\n",
    "                    break\n",
    "            \n",
    "        df.loc[k, 'num_' + class_label + '_events'] = len(cls_events)\n",
    "        df.loc[k, 'num_' + class_label + '_events_gtu_40'] = len(cls_events_gtu_40)\n",
    "        df.loc[k, 'num_' + class_label + '_packets'] = num_packets\n",
    "        df.loc[k, 'num_' + class_label + '_packets_gtu_40'] = num_packets_gtu_40\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN MODIFICATION !!! <strike>df['pack'] == 100 always</strike>\n",
    "# Changing back to pack - there are not 100 packets in a file\n",
    "\n",
    "pack_max_100 = np.min([df['pack'], np.ones(len(df))*100], axis=0)\n",
    "ones = np.ones(len(df))\n",
    "\n",
    "df = eusospb_analisi_with_pathnames_left_df\n",
    "\n",
    "for col in ['shower_packets', 'shower_packets_gtu_40']:\n",
    "    df['eff_' + col] = np.min([df['num_' + col] / pack_max_100, ones], axis=0)\n",
    "    \n",
    "df['shower_contamination'] =  df['num_shower_events'] / df['num_shower_packets_gtu_40']\n",
    "df['outside_gtu_40_packets'] = df['num_shower_packets'] - df['num_shower_packets_gtu_40']\n",
    "df['total_contamination'] =  (df['num_shower_events'] + df['num_noise_events']) / df['pack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eusospb_analisi_with_pathnames_left_df[[\n",
    "    'num_lens', 'run', 'per', 'mj', 'pack', \n",
    "    # 'euso_bal_ec2', 'euso_bal_ec5', 'euso_bal_ec8', \n",
    "    'max_euso_bal', \n",
    "    'eff_euso_bal', \n",
    "    'num_noise_events', \n",
    "    'num_noise_events_gtu_40', \n",
    "    'num_noise_packets', \n",
    "    'num_noise_packets_gtu_40',\n",
    "    'num_shower_events', \n",
    "    'num_shower_events_gtu_40',\n",
    "    'num_shower_packets', \n",
    "    'num_shower_packets_gtu_40',\n",
    "    'eff_shower_packets_gtu_40', \n",
    "    'outside_gtu_40_packets',\n",
    "    'shower_contamination', 'total_contamination',\n",
    "    'pack', 'acq_group', 'file_pathname'\n",
    "]].sort_values('mj', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eusospb_analisi_with_pathnames_left_df.query('num_noise_packets == 0 and num_shower_packets == 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics by acq group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def acq_group_stats(num_lens, acq_group=None, err_lims=(0.2,) ):\n",
    "    \n",
    "    \n",
    "    print('Num lens:', num_lens)\n",
    "    print('Acquisitions group:', acq_group)\n",
    "    \n",
    "    eusospb_analisi_2_len_by_mj = \\\n",
    "        eusospb_analisi_with_pathnames_left_df \\\n",
    "            .query('num_lens == {num_lens} and (num_noise_events > 0 or num_shower_events > 0) {acq_group_part}' \\\n",
    "                       .format(num_lens=num_lens, acq_group_part='and acq_group==\"{}\"'.format(acq_group) if acq_group else '')\n",
    "                  ) \\\n",
    "            .sort_values('mj', ascending=True)\n",
    "    \n",
    "    print('Number of entries:', len(eusospb_analisi_2_len_by_mj))\n",
    "    \n",
    "    IPython.display.display(eusospb_analisi_2_len_by_mj[[\n",
    "        'num_lens', 'run', 'per', 'mj', 'pack', \n",
    "        'max_euso_bal', 'max_euso_bal_10', \n",
    "        'num_noise_packets', 'num_shower_packets', \n",
    "        'num_noise_packets_gtu_40', 'num_shower_packets_gtu_40',\n",
    "        'eff_euso_bal_10', 'eff_shower_packets_gtu_40',\n",
    "        'acq_group', 'file_pathname'\n",
    "    ]])\n",
    "    \n",
    "    \n",
    "    pack_max_100 = np.min([eusospb_analisi_2_len_by_mj['pack'], np.ones(len(eusospb_analisi_2_len_by_mj))*100], axis=0)\n",
    "    num_shower_packets_gtu_40 = eusospb_analisi_2_len_by_mj['num_shower_packets_gtu_40'].data\n",
    "\n",
    "    mj_vals = eusospb_analisi_2_len_by_mj['mj']\n",
    "    \n",
    "    eff_euso_bal = eusospb_analisi_2_len_by_mj['eff_euso_bal']\n",
    "#     eff_euso_bal_10 = eusospb_analisi_2_len_by_mj['eff_euso_bal_10'].data\n",
    "    eff_shower_packets_gtu_40 = eusospb_analisi_2_len_by_mj['eff_shower_packets_gtu_40']\n",
    "        \n",
    "    yerr = statmodels_proportion_confint(\n",
    "        np.min([num_shower_packets_gtu_40, \n",
    "                np.ones(len(eusospb_analisi_2_len_by_mj))*100], axis=0), \n",
    "#       np.ones(len(eusospb_analisi_2_len_by_mj))*100,\n",
    "#       eusospb_analisi_2_len_by_mj['pack'].data,         # resored from 100\n",
    "        pack_max_100,\n",
    "        method='beta')\n",
    "    \n",
    "    # filtering\n",
    "    \n",
    "    for max_err in [None] + list(err_lims):\n",
    "        t_mj_vals = mj_vals\n",
    "        t_yerr = yerr\n",
    "        t_eff_euso_bal = eff_euso_bal\n",
    "#         t_eff_euso_bal_10 = eff_euso_bal_10\n",
    "        t_eff_shower_packets_gtu_40 = eff_shower_packets_gtu_40\n",
    "        max_err_suffix = ''\n",
    "        t_pack_max_100 = pack_max_100\n",
    "        \n",
    "        if max_err is not None:\n",
    "            max_err_suffix = '_max_err_{:.2f}'.format(max_err)\n",
    "            yerr_mask = (yerr[1] - yerr[0])  < max_err\n",
    "            t_mj_vals = np.array(mj_vals)[yerr_mask]\n",
    "            t_yerr = [yerr[0][yerr_mask], yerr[1][yerr_mask]]\n",
    "            t_eff_euso_bal = np.array(eff_euso_bal)[yerr_mask]\n",
    "#             t_eff_euso_bal_10 = eff_euso_bal_10[yerr_mask]\n",
    "            t_eff_shower_packets_gtu_40 = np.array(eff_shower_packets_gtu_40)[yerr_mask]\n",
    "            t_pack_max_100 = pack_max_100[yerr_mask]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(4,3))\n",
    "        ax.plot(t_mj_vals, t_eff_euso_bal, label='EUSO Bal. (P=1,R=1)')\n",
    "#         ax.plot(mj_vals, eff_euso_bal_10, label='EUSO Bal. (P=1,R=1) -10%')\n",
    "        ax.plot(t_mj_vals, t_eff_shower_packets_gtu_40, label='Classification model')  # .format(classification_id)) #  ({})\n",
    "\n",
    "        ax.legend()\n",
    "        ax.grid(axis='both', linestyle='--')\n",
    "        x_range = t_mj_vals.max() - t_mj_vals.min()\n",
    "        ax.set_xlim(t_mj_vals.min() - x_range*0.1, t_mj_vals.max() + x_range*0.1)\n",
    "        ax.set_xlabel('Laser energy [mJ]')\n",
    "        ax.set_ylabel('Efficiency')\n",
    "\n",
    "        combined_ylim = ax.get_ylim()\n",
    "        \n",
    "        path_compatible_acq_group = slugify(acq_group) if acq_group else 'any'\n",
    "        p = os.path.join(data_snippets_dir, \n",
    "                                 'efficiency_plot_acq_group_{}_eusobal_classification_model_comparison{}.svg' \\\n",
    "                                     .format(path_compatible_acq_group, str(max_err_suffix).replace('.','_')))\n",
    "        print(p)\n",
    "        fig.savefig(p)\n",
    "\n",
    "\n",
    "        for mj, eff, m, ye1, ye2 in zip(t_mj_vals, t_eff_shower_packets_gtu_40, t_pack_max_100, *t_yerr):\n",
    "            print('{:3.3f}\\t{:3.3f}\\t{:3.3f}\\t{:3.3f}\\t{:3.3f}'.format(mj, eff, m, ye1, ye2))\n",
    "\n",
    "        fig, ax, errbr = \\\n",
    "            plot_efficiency_stat_simple(\n",
    "                t_mj_vals, t_eff_shower_packets_gtu_40, \n",
    "                yerr=t_yerr,\n",
    "                figsize=(4,3), ylabel='Efficiency', xlabel='Laser energy [mJ]',\n",
    "                show=False\n",
    "            )\n",
    "        \n",
    "        ax.set_ylim(combined_ylim)\n",
    "        ax.set_xlim(t_mj_vals.min() - x_range*0.1, t_mj_vals.max() + x_range*0.1)\n",
    "        ax.grid(axis='both', linestyle='--')\n",
    "        \n",
    "        p = os.path.join(data_snippets_dir, \n",
    "                                 'efficiency_plot_acq_group_{}_classification_model{}.svg' \\\n",
    "                                     .format(path_compatible_acq_group, str(max_err_suffix).replace('.','_')))\n",
    "        print(p)\n",
    "        fig.savefig(p)\n",
    "        \n",
    "        \n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(5,2.6))\n",
    "        \n",
    "        fig, ax, errbr = \\\n",
    "            plot_efficiency_stat_simple(\n",
    "                t_mj_vals, t_eff_shower_packets_gtu_40, \n",
    "                yerr=t_yerr,\n",
    "                figsize=None, ylabel='Efficiency', xlabel='Laser energy [mJ]',\n",
    "                label='Extremly randomized trees classifier',\n",
    "                show=False,\n",
    "                ax=ax,\n",
    "                errorbar_attrs={**EFFICIENCY_STAT_ERRORBAR_DEFAULTS, 'marker': '.'}\n",
    "            )\n",
    "        \n",
    "        ax.plot(t_mj_vals, t_eff_euso_bal, label='First Level Trigger (P=1,R=1; 2 lens)', \n",
    "                color='red', marker='.', linestyle='-', alpha=.5,\n",
    "                zorder=99)\n",
    "        \n",
    "#         ax.plot(mj_vals, eff_euso_bal_10, label='EUSO Bal. (P=1,R=1) -10%')\n",
    "#         ax.plot(t_mj_vals, t_eff_shower_packets_gtu_40, label='Classification model')  # .format(classification_id)) #  ({})\n",
    "        \n",
    "        ax.legend()\n",
    "        ax.grid(axis='both', linestyle='--')\n",
    "        x_range = t_mj_vals.max() - t_mj_vals.min()\n",
    "        ax.set_ylim(0.4,1.1)\n",
    "        ax.set_xlim(t_mj_vals.min() - x_range*0.1, t_mj_vals.max() + x_range*0.1)\n",
    "        ax.set_xlabel('Laser energy [mJ]')\n",
    "        ax.set_ylabel('Efficiency')\n",
    "        \n",
    "        path_compatible_acq_group = slugify(acq_group) if acq_group else 'any'\n",
    "        p = os.path.join(data_snippets_dir, \n",
    "                                 'efficiency_plot_acq_group_{}_eusobal_classifier_w_errbars_range_04_11_comparison{}_526.svg' \\\n",
    "                                     .format(path_compatible_acq_group, str(max_err_suffix).replace('.','_')))\n",
    "        print(p)\n",
    "        fig.savefig(p)\n",
    "        \n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    x_range = mj_vals.max() - mj_vals.min()\n",
    "\n",
    "    print('Mean num packets outside GTU 40  ', eusospb_analisi_2_len_by_mj.outside_gtu_40_packets.mean())\n",
    "    print('Median num packets outside GTU 40', eusospb_analisi_2_len_by_mj.outside_gtu_40_packets.median())\n",
    "    \n",
    "    fig, ax, errbr = \\\n",
    "        plot_efficiency_stat_simple(\n",
    "            mj_vals, eusospb_analisi_2_len_by_mj.outside_gtu_40_packets,\n",
    "            figsize=(4,3), ylabel='Num packets outside GTU 40', xlabel='Laser energy [mJ]', yerr=None,\n",
    "            show=False\n",
    "        )\n",
    "\n",
    "    ax.set_xlim(mj_vals.min() - x_range*0.1, mj_vals.max() + x_range*0.1)\n",
    "    ax.grid(axis='both', linestyle='--')\n",
    "\n",
    "    p = os.path.join(data_snippets_dir, \n",
    "                             'outside_gtu_40_packets_plot_acq_group_{}_classification_model.svg' \\\n",
    "                                 .format(path_compatible_acq_group))\n",
    "    print(p)\n",
    "    fig.savefig(p)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    print('Mean trig events / GTU40 packets  ', eusospb_analisi_2_len_by_mj.shower_contamination.mean())\n",
    "    print('Median trig events / GTU40 packets', eusospb_analisi_2_len_by_mj.shower_contamination.median())\n",
    "    print('Mean trig events /pack', eusospb_analisi_2_len_by_mj.total_contamination.mean())\n",
    "    print('Median trig events /pack', eusospb_analisi_2_len_by_mj.total_contamination.median())\n",
    "    \n",
    "    fig, ax, errbr = \\\n",
    "        plot_efficiency_stat_simple(\n",
    "            mj_vals, eusospb_analisi_2_len_by_mj.shower_contamination,\n",
    "            figsize=(5,3), ylabel='Trig events / GTU40 packets', xlabel='Laser energy [mJ]', yerr=None,\n",
    "            show=False\n",
    "        )\n",
    "    \n",
    "    ax.set_xlim(mj_vals.min() - x_range*0.1, mj_vals.max() + x_range*0.1)\n",
    "    ax.grid(axis='both', linestyle='--')\n",
    "\n",
    "    p = os.path.join(data_snippets_dir, \n",
    "                             'contamination_plot_acq_group_{}_classification_model.svg' \\\n",
    "                                 .format(path_compatible_acq_group))\n",
    "    print(p)\n",
    "    fig.savefig(p)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    cls_events = utah_df_nonan[utah_df_nonan['source_file_acquisition'].isin(eusospb_analisi_2_len_by_mj['file_pathname'])]\n",
    "\n",
    "    cls_events_gtu_40 = cls_events[\n",
    "        ((cls_events.gtu_in_packet -4 + cls_events.num_gtu) > 40)  & \n",
    "        (cls_events.gtu_in_packet <= 45)\n",
    "     ]\n",
    "    cls_events_gtu_40_pack = cls_events_gtu_40 \\\n",
    "        .reset_index() \\\n",
    "        .sort_values(['packet_id', 'dist_gtu_40'], ascending=True) \\\n",
    "        .groupby('packet_id', as_index=False).first() \\\n",
    "        .set_index('index')\n",
    "    \n",
    "    for proba in np.arange(0.5, 1.0, 0.1):\n",
    "        print('p > {:.2f}: {}'.format(proba, np.count_nonzero(cls_events_gtu_40_pack[cls_proba_column] > proba)))\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(4,3))\n",
    "    cls_events_gtu_40_pack[cls_proba_column].hist(\n",
    "        ax=ax, bins=100, alpha=1, range=(0,1))\n",
    "    ax.set_ylabel('Number of events')\n",
    "    ax.set_xlabel('Probability')\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    \n",
    "    p = os.path.join(data_snippets_dir,  path_compatible_acq_group + '_events_gtu_40_pack_' + cls_proba_column +'_distribution_horizontal.svg')\n",
    "    \n",
    "    fig.savefig(p)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for num_lens in np.unique(eusospb_analisi_with_pathnames_left_df['num_lens']):\n",
    "    for acq_group in (None, *eusospb_analisi_with_pathnames_left_df.query('num_lens=='+str(num_lens))['acq_group'].unique().tolist()):\n",
    "        acq_group_stats(num_lens, acq_group)\n",
    "        print()\n",
    "        print('='*100)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low efficiency / failed entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def failed_mj(num_lens, acq_group=None, find_events=True, show_events=False, events_per_figure=10, gtu_range=(30, 45)):\n",
    "    \n",
    "    print('Num lens:', num_lens)\n",
    "    print('Acquisitions group:', acq_group)\n",
    "    \n",
    "    eusospb_analisi_2_len_by_mj = \\\n",
    "        eusospb_analisi_with_pathnames_left_df \\\n",
    "            .query('num_lens == {num_lens} and (num_noise_events > 0 or num_shower_events > 0) {acq_group_part} and eff_shower_packets_gtu_40 < 0.5 ' \\\n",
    "                       .format(num_lens=num_lens, acq_group_part='and acq_group==\"{}\"'.format(acq_group) if acq_group else '')\n",
    "                  ) \\\n",
    "            .sort_values('mj', ascending=True)\n",
    "    \n",
    "    print('Number of entries:', len(eusospb_analisi_2_len_by_mj))\n",
    "    \n",
    "    IPython.display.display(eusospb_analisi_2_len_by_mj[[\n",
    "        'num_lens', 'run', 'per', 'mj', 'pack', \n",
    "        'max_euso_bal', 'max_euso_bal_10', \n",
    "        'num_noise_packets', 'num_shower_packets', \n",
    "        'num_noise_packets_gtu_40', 'num_shower_packets_gtu_40',\n",
    "        'eff_euso_bal_10', 'eff_shower_packets_gtu_40',\n",
    "        'acq_group', 'file_pathname'\n",
    "    ]])\n",
    "    \n",
    "    if not find_events:\n",
    "        return\n",
    "\n",
    "    for i, r in eusospb_analisi_2_len_by_mj.iterrows():\n",
    "        file_pathname = r['file_pathname']\n",
    "        print(file_pathname)\n",
    "        print('\\teff_euso_bal = {:.3f}\\n\\teff_euso_bal_10 = {:.3f}\\n\\teff_shower_packets_gtu_40 = {:.3f}\\n\\tmj = {:.3f}\\n\\tpack = {:d}'.format(\n",
    "            r['eff_euso_bal'], r['eff_euso_bal_10'], r['eff_shower_packets_gtu_40'], r['mj'], r['pack']\n",
    "        ))\n",
    "#         all_file_entries = eusospb_analisi_with_pathnames_left_df.query('source_file_acquisition == \"{}\"'.format(file_pathname))\n",
    "#         print('Number of all entries:', len(all_file_entries))\n",
    "        \n",
    "        acq_events = utah_df_nonan[(utah_df_nonan['source_file_acquisition'] == file_pathname)]\n",
    "        \n",
    "        for class_num, class_label in enumerate(['noise', 'shower']):\n",
    "            \n",
    "            cls_events = acq_events[(acq_events[cls_column] == class_num)]\n",
    "            cls_packets_num_events = cls_events.groupby('packet_id')['event_id'].count()\n",
    "            \n",
    "            cls_events_gtu_40_df = \\\n",
    "                cls_events[\n",
    "                    (gtu_range[0] < cls_events['gtu_in_packet']) & \\\n",
    "                    (cls_events['gtu_in_packet'] < gtu_range[1])\n",
    "                ].sort_values(cls_proba_column, ascending=bool(class_num))\n",
    "            \n",
    "            cls_packets_num_events_gtu_40 = cls_events_gtu_40_df.groupby('packet_id')['event_id'].count()\n",
    "            \n",
    "            print()\n",
    "            print(\n",
    "                'Events classified as {class_label}:\\n\\tcount:        {count:5d} ({gtu_range[0]:d} < gtu_in_packet < {gtu_range[1]:d})\\n' \\\n",
    "                '\\tpacket count: {packet_count:5d} ({gtu_range[0]:d} < gtu_in_packet < {gtu_range[1]:d})' \\\n",
    "                    .format(\n",
    "                    class_label=class_label, \n",
    "                    count=len(cls_events_gtu_40_df), \n",
    "                    packet_count=len(cls_packets_num_events_gtu_40),\n",
    "                    gtu_range=gtu_range\n",
    "                )\n",
    "            )\n",
    "            print()\n",
    "            \n",
    "            if show_events:\n",
    "                vis_events_df(\n",
    "                    cls_events_gtu_40_df, \n",
    "                    events_per_figure=events_per_figure, max_figures=1, vis_gtux=True, vis_gtuy=True, \n",
    "                    close_after_vis=False, show=True, \n",
    "                    additional_printed_columns=[cls_proba_column, 'source_file_acquisition_full'],\n",
    "                    by_one=True,\n",
    "                    extension_func=None,\n",
    "                    single_proj_width=4, single_proj_height=3\n",
    "                )\n",
    "            print('-'*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num_lens in np.unique(eusospb_analisi_with_pathnames_left_df['num_lens']):\n",
    "    for acq_group in eusospb_analisi_with_pathnames_left_df.query('num_lens=='+str(num_lens))['acq_group'].unique().tolist():\n",
    "        failed_mj(num_lens, acq_group, find_events=True, show_events=True)\n",
    "        print()\n",
    "        print('='*100)\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Examination of `300916/GLS/allpackets-SPBEUSO-ACQUISITION-20161004-041003-001.001--45degaway36per.root`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- conclusion: all seems to be fine\n",
    "- file has 39 packets (as pack column)\n",
    "- 35 packet are correctly recognized showers (as note in the file)\n",
    "- 2 packets are noise and are misclassified as an air shower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eusospb_analisi_with_pathnames_left_df[eusospb_analisi_with_pathnames_left_df.file_pathname == '300916/GLS/allpackets-SPBEUSO-ACQUISITION-20161004-041003-001.001--45degaway36per.root']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cls_events_df = utah_df_nonan[(utah_df_nonan['source_file_acquisition'] == '300916/GLS/allpackets-SPBEUSO-ACQUISITION-20161004-041003-001.001--45degaway36per.root')]\n",
    "# & (utah_df_nonan[cls_column] == class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cls_events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(cls_events_df.groupby('packet_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "positive_cls_events_df = cls_events_df[(cls_events_df[cls_column] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(positive_cls_events_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "positive_cls_events_gtu_40_df = positive_cls_events_df[(positive_cls_events_df.gtu_in_packet < 45) & (positive_cls_events_df.gtu_in_packet >= 30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "positive_cls_events_gtu_non_40_df = positive_cls_events_df[(positive_cls_events_df.gtu_in_packet >= 45) | (positive_cls_events_df.gtu_in_packet < 30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(positive_cls_events_gtu_40_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(positive_cls_events_gtu_non_40_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_events_df(\n",
    "    positive_cls_events_gtu_40_df, \n",
    "    events_per_figure=40, max_figures=1, vis_gtux=True, vis_gtuy=True, \n",
    "    close_after_vis=False, show=True, \n",
    "    additional_printed_columns=[cls_proba_column, 'source_file_acquisition_full'],\n",
    "    by_one=True,\n",
    "    extension_func=None,\n",
    "    single_proj_width=4, single_proj_height=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vis_events_df(\n",
    "    positive_cls_events_gtu_non_40_df, \n",
    "    events_per_figure=40, max_figures=1, vis_gtux=True, vis_gtuy=True, \n",
    "    close_after_vis=False, show=True, \n",
    "    additional_printed_columns=[cls_proba_column, 'source_file_acquisition_full'],\n",
    "    by_one=True,\n",
    "    extension_func=None,\n",
    "    single_proj_width=4, single_proj_height=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examination of `041016/GLS/allpackets-SPBEUSO-ACQUISITION-20161004-041003-001.001--45degaway36per.root`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = '041016/GLS/allpackets-SPBEUSO-ACQUISITION-20161004-041003-001.001--45degaway36per.root'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eusospb_analisi_with_pathnames_left_df[eusospb_analisi_with_pathnames_left_df.file_pathname == f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_events_df = utah_df_nonan[(utah_df_nonan['source_file_acquisition'] == f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_events_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquisition groups for all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_acq_groups_pathname = os.path.join(utah_file_analysis_snippets_dir, 'files_acq_groups.tsv')\n",
    "files_acq_groups_df = pd.read_csv(files_acq_groups_pathname, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_acq_groups_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utah_df_nonan_w_acq_groups_w_mj_df = \\\n",
    "    pd.merge(\n",
    "        pd.merge(utah_df_nonan, files_acq_groups_df, how='outer', left_on=['source_file_acquisition'], right_on=['file_pathname']),\n",
    "        eusospb_analisi_with_pathnames_left_df[['file_pathname', 'num_lens', 'mj', 'pack']],\n",
    "        how='left',\n",
    "        on=['file_pathname'], \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utah_df_nonan_w_acq_groups_w_mj_df[[\n",
    "    'source_file_acquisition', \n",
    "     'entries',\n",
    "    cls_column,\n",
    "    'acq_group_l0', 'acq_group_l1', 'acq_group_l2', 'run', \n",
    "    'num_lens', 'mj'\n",
    "]][(~utah_df_nonan_w_acq_groups_w_mj_df['source_file_acquisition'].isnull()) & \n",
    "   (~utah_df_nonan_w_acq_groups_w_mj_df[cls_column].isnull())] # 'pack', 'entries',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utah_df_nonan_w_acq_groups_w_mj_df[~utah_df_nonan_w_acq_groups_w_mj_df.mj.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utah_classified_entries_mask = (~utah_df_nonan_w_acq_groups_w_mj_df['source_file_acquisition'].isnull()) & (~utah_df_nonan_w_acq_groups_w_mj_df[cls_column].isnull())\n",
    "utah_df_nonan_w_acq_groups_w_mj_classied_df = utah_df_nonan_w_acq_groups_w_mj_df[utah_classified_entries_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('acq_group_l0', np.count_nonzero(utah_df_nonan_w_acq_groups_w_mj_classied_df.acq_group_l0.isnull()))\n",
    "print('acq_group_l1', np.count_nonzero(utah_df_nonan_w_acq_groups_w_mj_classied_df.acq_group_l1.isnull()))\n",
    "print('acq_group_l2', np.count_nonzero(utah_df_nonan_w_acq_groups_w_mj_classied_df.acq_group_l2.isnull()))\n",
    "print('run', np.count_nonzero(utah_df_nonan_w_acq_groups_w_mj_classied_df.run.isnull()))\n",
    "print('num_lens', np.count_nonzero(utah_df_nonan_w_acq_groups_w_mj_classied_df.num_lens.isnull()))\n",
    "print('mj', np.count_nonzero(utah_df_nonan_w_acq_groups_w_mj_classied_df.mj.isnull()))\n",
    "print('entries', np.count_nonzero(utah_df_nonan_w_acq_groups_w_mj_classied_df.entries.isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the classification results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of data for the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utah_df_nonan_w_acq_groups_w_mj_classied_for_db_df = \\\n",
    "    utah_df_nonan_w_acq_groups_w_mj_classied_df[[\n",
    "        'event_id',\n",
    "        'source_file_acquisition', \n",
    "         'entries',\n",
    "        cls_column,\n",
    "        cls_proba_column,\n",
    "        'acq_group_l0', 'acq_group_l1', 'acq_group_l2', 'run', \n",
    "        'num_lens', 'mj'\n",
    "    ]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len\\\n",
    "(\n",
    "np.unique( ((utah_df_nonan_w_acq_groups_w_mj_classied_for_db_df.entries // 128) // 50) * 50 )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utah_df_nonan_w_acq_groups_w_mj_classied_for_db_df['packets_approx'] = \\\n",
    "    ((utah_df_nonan_w_acq_groups_w_mj_classied_for_db_df.entries // 128) // 50) * 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_fillna_columns = ['num_lens', 'mj']\n",
    "str_fillna_columns = ['acq_group_l1', 'acq_group_l2']\n",
    "\n",
    "utah_df_nonan_w_acq_groups_w_mj_classied_for_db_df[numeric_fillna_columns] = \\\n",
    "    utah_df_nonan_w_acq_groups_w_mj_classied_for_db_df[numeric_fillna_columns].fillna(-1)\n",
    "\n",
    "utah_df_nonan_w_acq_groups_w_mj_classied_for_db_df[str_fillna_columns] = \\\n",
    "    utah_df_nonan_w_acq_groups_w_mj_classied_for_db_df[str_fillna_columns].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utah_df_nonan_w_acq_groups_w_mj_classied_for_db_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_classification_slug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_columns = collections.OrderedDict([\n",
    "    (event_v3_storage_provider_utah.data_table_pk, int),\n",
    "    (cls_column, int), \n",
    "    (cls_proba_column, float),\n",
    "    ('source_file_acquisition', str), \n",
    "    ('entries', int),\n",
    "    ('packets_approx', int),\n",
    "    ('acq_group_l0', str), \n",
    "    ('acq_group_l1', str), \n",
    "    ('acq_group_l2', str), \n",
    "    ('run', int), \n",
    "    ('num_lens', int), \n",
    "    ('mj', float)\n",
    "])\n",
    "\n",
    "print(\n",
    "    event_v3_storage_provider_utah.connection.cursor().mogrify(\n",
    "        event_v3_storage_provider_utah.get_classification_table_query(subset_classification_slug, classification_columns.items())\n",
    "    ).decode()\n",
    ")\n",
    "\n",
    "event_v3_storage_provider_utah.create_classification_table(subset_classification_slug, classification_columns.items())\n",
    "\n",
    "event_v3_storage_provider_utah.save_classification_data(\n",
    "    subset_classification_slug, \n",
    "    utah_df_nonan_w_acq_groups_w_mj_classied_for_db_df[list(classification_columns.keys())].values, \n",
    "    classification_columns.items(), \n",
    "    num_inserts_at_once=1000, morgify=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification_column_names = [col for col in flight_df_gtu_36_45_pack_nonan.columns \\\n",
    "#                                if col.startswith('tsne_') or \\\n",
    "#                                        col == cls_column or col == cls_proba_column or \\\n",
    "#                                        col == event_v3_storage_provider_flight.data_table_pk]\n",
    "# classification_column_type = [(int \\\n",
    "#                                if col == cls_column or col.endswith('dbscan_y') or \\\n",
    "#                                        col == event_v3_storage_provider_flight.data_table_pk or \\\n",
    "#                                        col == event_v3_storage_provider_flight.data_table_pk \\\n",
    "#                                    else float) \\\n",
    "#                               for col in classification_column_names]\n",
    "\n",
    "# classification_columns = list(zip(classification_column_names, classification_column_type))\n",
    "\n",
    "\n",
    "# print('-'*50)\n",
    "# print(subset_classification_slug)\n",
    "# print(classification_column_names)\n",
    "\n",
    "# event_v3_storage_provider_flight.create_classification_table(subset_classification_slug, classification_columns)\n",
    "\n",
    "# event_v3_storage_provider_flight.save_classification_data(\n",
    "#     subset_classification_slug, \n",
    "#     flight_df_gtu_36_45_pack_nonan[classification_column_names].values, classification_columns, \n",
    "#     num_inserts_at_once=1000, morgify=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#event_v3_storage_provider_flight.connection.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flight_nonan_classified_shower_pathname = os.path.join(data_snippets_dir, 'flight_nonan_classified shower.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flight_df_nonan[flight_df_nonan['extra_trees_cls_on_train_kbest400_128_est_dropna']==1].to_csv(flight_nonan_classified_shower_pathname, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO select clusters with positive classification (sort by the number of classifications) show distribution of event types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# flight_df_nonan_subset[['dbscan_tsne_y','manual_classification_class_number'].hist('dbscan_tsne_y', figsize=(24,4), bins=2*len(dbscan_on_tsne_classes)+1)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS NOT WHAT IS DESIRED - values should be split into features ?\n",
    "# flight_nonan__cls_tsneclu_corr_df = \\\n",
    "#     flight_df_nonan[['dbscan_tsne_y', 'manual_classification_class_number']].corr()\n",
    "# f, ax = plt.subplots(figsize=(28,22))\n",
    "# plt.close('all')\n",
    "# sns.heatmap(flight_nonan__cls_tsneclu_corr_df, cmap='inferno', annot=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     f, ax = plt.subplots()\n",
    "#     f.set_size_inches(8,4)\n",
    "#     flight_df_nonan_subset[['dbscan_tsne_y', 'manual_classification_class_number']].plot.bar(by='dbscan_tsne_y', ax=ax)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flight_nonan__tsne__gmm_y_pred = gmm.predict(flight_df_nonan[['tsne_X_0','tsne_X_1']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flight_data__k50best_var_th_scaled_X = \\\n",
    "#     k50best_f_classif_selector_on_var_th_sc_train.transform(\n",
    "#         var_th_selector_on_scaled_train.transform(\n",
    "#             standard_scaler_on_train.transform(\n",
    "#                 unl_flight_df[analyzed_common_df_columns].dropna().values)\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# extra_trees_classifier_on_train_kbest50__X_flight = flight_data__k50best_var_th_scaled_X\n",
    "# extra_trees_classifier_on_train_kbest50__y_flight_pred = \\\n",
    "#     extra_trees_classifier_on_train_kbest50.predict(extra_trees_classifier_on_train_kbest50__X_flight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
